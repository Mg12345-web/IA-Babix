from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import chromadb
import os
import re
from openai import OpenAI
import tiktoken

router = APIRouter()

# Configura√ß√µes principais
CHROMA_DIR = os.getenv("CHROMA_DIR", "./dados/chroma")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Cliente OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)

# Token counter
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def get_chroma_client():
    """Conecta ao ChromaDB persistente"""
    return chromadb.PersistentClient(path=CHROMA_DIR)

def get_embedder():
    """Carrega modelo de embedding"""
    return SentenceTransformer("all-MiniLM-L6-v2")

def truncate_text(text, max_tokens=1000):
    """Trunca texto para n√£o exceder max_tokens"""
    tokens = encoding.encode(text)
    if len(tokens) > max_tokens:
        truncated = encoding.decode(tokens[:max_tokens])
        return truncated + "..."
    return text

def extract_codes(query):
    """Extrai c√≥digos/n√∫meros da query (ex: 516-91, art 165)"""
    codes = re.findall(r'\d{3}-\d{2}|\d{1,3}', query)
    return codes

class ChatRequest(BaseModel):
    message: str

@router.post("/chat")
async def chat(req: ChatRequest):
    """
    Endpoint de chat com RAG otimizado
    Prioriza buscas por c√≥digo quando detecta n√∫meros
    """
    try:
        query = req.message.strip()
        
        if not query:
            raise HTTPException(status_code=400, detail="Mensagem vazia.")
        
        # üîç Conectar ao ChromaDB
        chroma_client = get_chroma_client()
        
        # Tentar obter a cole√ß√£o existente
        try:
            collection = chroma_client.get_collection("babix_docs")
        except Exception as e:
            print(f"‚ùå Cole√ß√£o n√£o encontrada: {e}")
            return {
                "response": "‚ö†Ô∏è Nenhum documento foi indexado ainda. Clique em 'Fazer Ingest√£o' primeiro."
            }
        
        # Verificar quantos documentos est√£o indexados
        count = collection.count()
        print(f"üìö Documentos na cole√ß√£o: {count}")
        
        if count == 0:
            return {
                "response": "‚ö†Ô∏è Cole√ß√£o vazia. Fa√ßa a ingest√£o de PDFs primeiro."
            }
        
        # üîç Se houver n√∫meros/c√≥digos, melhorar query
        codes = extract_codes(query)
        if codes:
            print(f"üî¢ C√≥digos detectados: {codes}")
            # Enriquecer a query com termos relacionados
            query_enriched = query
            for code in codes:
                query_enriched += f" c√≥digo {code} infra√ß√£o artigo"
        else:
            query_enriched = query
        
        # üîç Buscar documentos similares (pega 3 para ter mais contexto)
        embedder = get_embedder()
        query_embedding = embedder.encode(query_enriched)
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3  # Aumentado para 3 para melhor contexto
        )
        
        # Verificar se encontrou resultados
        if not results or not results.get("documents") or len(results["documents"][0]) == 0:
            print("‚ö†Ô∏è Nenhum documento similar encontrado")
            return {
                "response": "Desculpe, n√£o encontrei informa√ß√µes sobre sua pergunta nos documentos indexados."
            }
        
        # üìÑ Extrair contextos e truncar
        documents = results["documents"][0]
        metadatas = results["metadatas"][0] if results.get("metadatas") else []
        
        # Truncar cada documento para max 800 tokens
        truncated_docs = [truncate_text(doc, max_tokens=800) for doc in documents]
        context = "\n\n---\n\n".join(truncated_docs)
        
        # Verificar tamanho do contexto
        context_tokens = len(encoding.encode(context))
        print(f"üìä Tokens do contexto: {context_tokens}")
        
        if context_tokens > 2000:
            context = truncate_text(context, max_tokens=1500)
            print("‚ö†Ô∏è Contexto truncado para 1500 tokens")
        
        # ü§ñ Chamar GPT com contexto
        system_message = """Voc√™ √© um assistente jur√≠dico especializado em direito de tr√¢nsito brasileiro.
Use as informa√ß√µes dos documentos fornecidos para responder.
Se tiver m√∫ltiplos documentos sobre o mesmo c√≥digo, unifique a informa√ß√£o mais precisa.
Seja conciso e objetivo. M√°ximo 200 palavras."""
        
        user_message = f"""Documentos relevantes:
{context}

Pergunta: {query}

Responda com base nos documentos acima. Se houver conflito entre informa√ß√µes, use a mais recente ou da lei (CTB) em vez de resolu√ß√µes."""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=0.3,  # Reduzido para mais precis√£o
            max_tokens=300
        )
        
        answer = response.choices[0].message.content
        
        # Adicionar fontes
        sources = [m.get("name", "Documento")[:50] for m in metadatas if m]
        sources_text = f"\n\nüìö Fontes: {', '.join(set(sources))}" if sources else ""
        
        return {
            "response": answer + sources_text
        }
        
    except Exception as e:
        print(f"‚ùå Erro no chat: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            "response": f"Desculpe, tivemos um erro ao processar sua pergunta. Tente novamente em alguns segundos."
        }
