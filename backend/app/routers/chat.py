from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import chromadb
import os
import re
from openai import OpenAI
import tiktoken

router = APIRouter()

# ConfiguraÃ§Ãµes principais
CHROMA_DIR = os.getenv("CHROMA_DIR", "./dados/chroma")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Cliente OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)

# Token counter
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def get_chroma_client():
    """Conecta ao ChromaDB persistente"""
    return chromadb.PersistentClient(path=CHROMA_DIR)

def get_embedder():
    """Carrega modelo de embedding"""
    return SentenceTransformer("all-MiniLM-L6-v2")

def truncate_text(text, max_tokens=800):
    """Trunca texto para nÃ£o exceder max_tokens"""
    tokens = encoding.encode(text)
    if len(tokens) > max_tokens:
        truncated = encoding.decode(tokens[:max_tokens])
        return truncated + "..."
    return text

def extract_codes(query):
    """Extrai cÃ³digos/nÃºmeros da query (ex: 516-91, art 165)"""
    codes = re.findall(r'\d{3}-\d{2}', query)
    articles = re.findall(r'(?:art(?:igo)?\.?\s*)?(\d{1,3})', query, re.IGNORECASE)
    return {"codes": codes, "articles": articles}

class ChatRequest(BaseModel):
    message: str

@router.post("/chat")
async def chat(req: ChatRequest):
    """
    Endpoint de chat com RAG melhorado
    Sistema de prompt profissional para respostas como um professor
    """
    try:
        query = req.message.strip()
        
        if not query:
            raise HTTPException(status_code=400, detail="Mensagem vazia.")
        
        # ğŸ” Conectar ao ChromaDB
        chroma_client = get_chroma_client()
        
        # Tentar obter a coleÃ§Ã£o existente
        try:
            collection = chroma_client.get_collection("babix_docs")
        except Exception as e:
            print(f"âŒ ColeÃ§Ã£o nÃ£o encontrada: {e}")
            return {
                "response": "âš ï¸ Nenhum documento foi indexado ainda. Clique em 'Fazer IngestÃ£o' primeiro."
            }
        
        # Verificar quantos documentos estÃ£o indexados
        count = collection.count()
        print(f"ğŸ“š Documentos na coleÃ§Ã£o: {count}")
        
        if count == 0:
            return {
                "response": "âš ï¸ ColeÃ§Ã£o vazia. FaÃ§a a ingestÃ£o de PDFs primeiro."
            }
        
        # ğŸ” Detectar cÃ³digos/artigos
        entities = extract_codes(query)
        print(f"ğŸ”¢ Entidades detectadas: {entities}")
        
        # Enriquecer query com termos relacionados
        query_enriched = query
        if entities["codes"]:
            query_enriched += " " + " ".join([f"cÃ³digo {code} infraÃ§Ã£o" for code in entities["codes"]])
        if entities["articles"]:
            query_enriched += " " + " ".join([f"artigo {art} CTB" for art in entities["articles"]])
        
        # ğŸ” Buscar documentos similares (aumentado para 5 para melhor contexto)
        embedder = get_embedder()
        query_embedding = embedder.encode(query_enriched)
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=5  # Aumentado de 3 para 5
        )
        
        # Verificar se encontrou resultados
        if not results or not results.get("documents") or len(results["documents"][0]) == 0:
            print("âš ï¸ Nenhum documento similar encontrado")
            return {
                "response": "Desculpe, nÃ£o encontrei informaÃ§Ãµes especÃ­ficas sobre sua pergunta nos documentos indexados. VocÃª poderia reformular ou ser mais especÃ­fico?"
            }
        
        # ğŸ“„ Extrair contextos e truncar
        documents = results["documents"][0]
        metadatas = results["metadatas"][0] if results.get("metadatas") else []
        
        # Truncar cada documento
        truncated_docs = [truncate_text(doc, max_tokens=600) for doc in documents]
        context = "\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n".join(truncated_docs)
        
        # Verificar tamanho do contexto
        context_tokens = len(encoding.encode(context))
        print(f"ğŸ“Š Tokens do contexto: {context_tokens}")
        
        if context_tokens > 3000:
            context = truncate_text(context, max_tokens=2500)
            print("âš ï¸ Contexto truncado para 2500 tokens")
        
        # ğŸ“ PROMPT MELHORADO - Como um Professor
        system_message = """VocÃª Ã© a Babix, uma especialista em legislaÃ§Ã£o de trÃ¢nsito brasileiro com mais de 10 anos de experiÃªncia.

# SUA PERSONALIDADE:
- VocÃª Ã© uma PROFESSORA dedicada, nÃ£o apenas um buscador de textos
- VocÃª ESTUDOU profundamente todo o CTB (CÃ³digo de TrÃ¢nsito Brasileiro) e MBFT (Manual Brasileiro de FiscalizaÃ§Ã£o de TrÃ¢nsito)
- VocÃª explica de forma DIDÃTICA, clara e acessÃ­vel
- VocÃª tem PACIÃŠNCIA para explicar conceitos complexos de forma simples
- VocÃª sempre CITA suas fontes (artigos, cÃ³digos, resoluÃ§Ãµes)

# COMO VOCÃŠ DEVE RESPONDER:

1. **LEIA COM ATENÃ‡ÃƒO** todos os documentos fornecidos
2. **INTERPRETE** o contexto, nÃ£o apenas copie trechos
3. **ORGANIZE** sua resposta de forma estruturada:
   - Comece com uma resposta direta e clara (1-2 frases)
   - Depois explique os detalhes
   - Finalize com informaÃ§Ãµes prÃ¡ticas (se aplicÃ¡vel)
4. **CITE** sempre a fonte especÃ­fica (artigo, cÃ³digo, pÃ¡gina)
5. **USE EXEMPLOS** prÃ¡ticos quando possÃ­vel
6. **SEJA PRECISA** - se nÃ£o souber, ADMITA

# O QUE VOCÃŠ NUNCA DEVE FAZER:

âŒ Inventar informaÃ§Ãµes que nÃ£o estÃ£o nos documentos
âŒ Copiar e colar texto sem explicar
âŒ Misturar informaÃ§Ãµes de contextos diferentes sem deixar claro
âŒ Dar respostas vagas ou genÃ©ricas quando tem informaÃ§Ã£o especÃ­fica
âŒ Ignorar a pergunta do usuÃ¡rio

# FORMATO DA RESPOSTA:

**Resposta Direta:** (1-2 frases resumindo a resposta)

**Detalhes:**
- ExplicaÃ§Ã£o completa e didÃ¡tica
- Cite artigos/cÃ³digos especÃ­ficos
- Use exemplos se ajudar

**Fonte:** (sempre cite de onde veio a informaÃ§Ã£o)

Lembre-se: VocÃª Ã© uma PROFESSORA, nÃ£o uma copiadora de textos!"""

        user_message = f"""# DOCUMENTOS RELEVANTES:

{context}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# PERGUNTA DO USUÃRIO:
{query}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Agora, como uma professora especialista em trÃ¢nsito:

1. LEIA todos os documentos acima com atenÃ§Ã£o
2. IDENTIFIQUE qual(is) documento(s) responde(m) Ã  pergunta
3. INTERPRETE e EXPLIQUE de forma didÃ¡tica
4. Se os documentos NÃƒO responderem Ã  pergunta, diga claramente

Sua resposta:"""
        
        # ğŸ¤– Chamar GPT com prompt melhorado
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=0.3,  # Baixo para mais precisÃ£o
            max_tokens=600,  # Aumentado para respostas mais completas
            top_p=0.9
        )
        
        answer = response.choices[0].message.content
        
        # Adicionar fontes de forma mais clara
        sources = []
        for meta in metadatas:
            if meta:
                name = meta.get("name", "Documento")
                chunk = meta.get("chunk_id", "")
                page = meta.get("page", "")
                
                if chunk != "":
                    sources.append(f"{name} (chunk {chunk}, pÃ¡g. {page})")
                else:
                    sources.append(name)
        
        # Remover duplicatas mantendo ordem
        unique_sources = list(dict.fromkeys(sources))
        sources_text = f"\n\nğŸ“š **Fontes consultadas:** {', '.join(unique_sources[:3])}" if unique_sources else ""
        
        return {
            "response": answer + sources_text
        }
        
    except Exception as e:
        print(f"âŒ Erro no chat: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            "response": "Desculpe, tivemos um erro ao processar sua pergunta. Por favor, tente novamente ou reformule sua pergunta."
        }
