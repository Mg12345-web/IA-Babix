from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import chromadb
import os
from openai import OpenAI
import tiktoken

router = APIRouter()

# Configura√ß√µes principais
CHROMA_DIR = os.getenv("CHROMA_DIR", "./dados/chroma")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Cliente OpenAI
client = OpenAI(api_key=OPENAI_API_KEY)

# Token counter
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")

def get_chroma_client():
    """Conecta ao ChromaDB persistente"""
    return chromadb.PersistentClient(path=CHROMA_DIR)

def get_embedder():
    """Carrega modelo de embedding"""
    return SentenceTransformer("all-MiniLM-L6-v2")

def truncate_text(text, max_tokens=1000):
    """Trunca texto para n√£o exceder max_tokens"""
    tokens = encoding.encode(text)
    if len(tokens) > max_tokens:
        truncated = encoding.decode(tokens[:max_tokens])
        return truncated + "..."
    return text

class ChatRequest(BaseModel):
    message: str

@router.post("/chat")
async def chat(req: ChatRequest):
    """
    Endpoint de chat com RAG otimizado
    """
    try:
        query = req.message.strip()
        
        if not query:
            raise HTTPException(status_code=400, detail="Mensagem vazia.")
        
        # üîç Conectar ao ChromaDB
        chroma_client = get_chroma_client()
        
        # Tentar obter a cole√ß√£o existente
        try:
            collection = chroma_client.get_collection("babix_docs")
        except Exception as e:
            print(f"‚ùå Cole√ß√£o n√£o encontrada: {e}")
            return {
                "response": "‚ö†Ô∏è Nenhum documento foi indexado ainda. Clique em 'Fazer Ingest√£o' primeiro."
            }
        
        # Verificar quantos documentos est√£o indexados
        count = collection.count()
        print(f"üìö Documentos na cole√ß√£o: {count}")
        
        if count == 0:
            return {
                "response": "‚ö†Ô∏è Cole√ß√£o vazia. Fa√ßa a ingest√£o de PDFs primeiro."
            }
        
        # üîç Buscar documentos similares (pega apenas 2 para economizar tokens)
        embedder = get_embedder()
        query_embedding = embedder.encode(query)
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=2  # Reduzido de 3 para 2
        )
        
        # Verificar se encontrou resultados
        if not results or not results.get("documents") or len(results["documents"][0]) == 0:
            print("‚ö†Ô∏è Nenhum documento similar encontrado")
            return {
                "response": "Desculpe, n√£o encontrei informa√ß√µes relevantes sobre sua pergunta."
            }
        
        # üìÑ Extrair contextos e truncar
        documents = results["documents"][0]
        metadatas = results["metadatas"][0] if results.get("metadatas") else []
        
        # Truncar cada documento para max 800 tokens
        truncated_docs = [truncate_text(doc, max_tokens=800) for doc in documents]
        context = "\n\n---\n\n".join(truncated_docs)
        
        # Verificar tamanho do contexto
        context_tokens = len(encoding.encode(context))
        print(f"üìä Tokens do contexto: {context_tokens}")
        
        if context_tokens > 2000:
            # Se ainda for grande, truncar o contexto inteiro
            context = truncate_text(context, max_tokens=1500)
            print("‚ö†Ô∏è Contexto muito grande, truncado para 1500 tokens")
        
        # ü§ñ Chamar GPT com contexto (RAG)
        system_message = """Voc√™ √© um assistente jur√≠dico especializado em direito de tr√¢nsito brasileiro.
Use as informa√ß√µes dos documentos fornecidos para responder.
Seja conciso e objetivo. M√°ximo 200 palavras."""
        
        user_message = f"""Informa√ß√µes dos documentos:
{context}

Pergunta do usu√°rio: {query}

Responda com base nas informa√ß√µes acima."""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=0.5,
            max_tokens=300  # Reduzido para economizar
        )
        
        answer = response.choices[0].message.content
        
        # Adicionar fontes
        sources = [m.get("name", "Documento")[:40] for m in metadatas if m]  # Apenas 40 chars
        sources_text = f"\n\nüìö Fontes: {', '.join(set(sources))}" if sources else ""
        
        return {
            "response": answer + sources_text
        }
        
    except Exception as e:
        print(f"‚ùå Erro no chat: {str(e)}")
        
        return {
            "response": f"Desculpe, tivemos um erro ao processar sua pergunta. Tente novamente em alguns segundos."
        }
